---
title: "Analyzing the running time performance of PyTorch, `torch`, and `mlr3torch`"
description: |
  Compare the running time performance of PyTorch, `torch`, and `mlr3torch`.
format:
  html:
    embed-resources: true
---

```{r include = FALSE}
library(here)
library(readr)
library(magrittr)
library(dplyr)
```

```{r include = FALSE}
source(here("R", "output_dir_name.R"))
```

```{r include = FALSE}
# TODO: figure out why this doesn't work when you run a second time
result_dir_name = result_dir_name()
result_file_name = here(result_dir_name, "benchmark_results.csv")
benchmark_results = read_csv(result_file_name) %>%
  select(library, total_time)
```

## Setup

We use the `benchmark` package to measure the time required for training models. This process is applied to the ["Guess the correlation"](https://torch.mlverse.org/start/guess_the_correlation/) dataset.

```{r echo = FALSE}
knitr::kable(
  x = benchmark_results,
  digits = 2,
  col.names = c("Library", "Running time (seconds)"))
```

``` {.yaml filename="config.yml"}
default:
  architecture_id: "cnn"
  accelerator: "cuda"
  n_epochs: 5
  batch_size: 64
  learning_rate: 0.005
  train_size: 1000
```

Manually copy in the `config.yml` file from the appropriate results directory.

